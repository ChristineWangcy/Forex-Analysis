{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "profile currency analysis 1.2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Sf0EmPzEmGx4"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristineWangcy/Forex-Analysis/blob/main/profile_currency_analysis_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCc3XZEyG3XV"
      },
      "source": [
        "# Predict next5dayss change for currency EUNO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBNaLR1UOrol"
      },
      "source": [
        "currency = 'EUNO'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8453JSl4uYIb"
      },
      "source": [
        "# install and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sX_xtn57WTo"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "!pip install category_encoders==2.*\n",
        "!pip install pdpbox\n",
        "!pip install shap\n",
        "!pip install --upgrade numpy==1.19.1\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning, module='xgboost')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Kwz804WIdl"
      },
      "source": [
        "# data analysis and wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# encoders\n",
        "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_score, recall_score, r2_score, \\\n",
        " classification_report, roc_auc_score, plot_confusion_matrix, classification_report\n",
        "\n",
        "# pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# machine learning\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Boosted Models\n",
        "# Use this one if you have an M1 chip.\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "\n",
        "# Permutation Importance\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# for displaying images and html\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "\n",
        "# Partial Dependence Plot\n",
        "from pdpbox.pdp import pdp_isolate, pdp_plot, pdp_interact, pdp_interact_plot\n",
        "\n",
        "# shap\n",
        "import shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6KJ7OuYx9kj"
      },
      "source": [
        "# Wrangling data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_3gggdeyg7"
      },
      "source": [
        "def wrangle(file):\n",
        "  df = pd.read_csv(file, parse_dates=['DATE'], index_col=0)\n",
        "  df.dropna(inplace=True)\n",
        "  df = df.sort_index(ascending=True)\n",
        "  df = df.applymap(lambda x: float(x))\n",
        "  if file[5:7] == currency[:2]:\n",
        "    df['close'] = 1/df['close']\n",
        "  print('------file started: ', file)\n",
        "  # add more features\n",
        "  df['change'] = df['close'] / df['close'].shift(1) - 1\n",
        "  df['5days_change'] = df['close']/df['close'].shift(5) - 1\n",
        "  df['next5days_change'] = df['close'].shift(-5)/df['close'] - 1\n",
        "  #df['nextday_change'] = df['change'].shift(-1)\n",
        "  df['bias_5days_ave'] = df['close']/df['close'].rolling(window=5).mean() - 1\n",
        "  df['bias_10days_ave'] = df['close']/df['close'].rolling(window=10).mean() - 1\n",
        "  df['bias_30days_ave'] = df['close']/df['close'].rolling(window=30).mean() - 1\n",
        "  df['bias_60days_ave'] = df['close']/df['close'].rolling(window=60).mean() - 1\n",
        "  \n",
        "  df['max_3days'] = (df['close'] == df['close'].rolling(window=3).max()).astype(int)\n",
        "  df['max_5days'] = (df['close'] == df['close'].rolling(window=5).max()).astype(int)\n",
        "  df['max_10days'] = (df['close'] == df['close'].rolling(window=10).max()).astype(int)\n",
        "  df['min_3days'] = (df['close'] == df['close'].rolling(window=3).min()).astype(int)\n",
        "  df['min_5days'] = (df['close'] == df['close'].rolling(window=5).min()).astype(int)\n",
        "  df['min_10days'] = (df['close'] == df['close'].rolling(window=10).min()).astype(int)\n",
        "  \n",
        "  is_10days_max = ((df['close'] == df['close'].rolling(window=6).max()) & (df['close'] == df['close'].shift(-5).rolling(window=6).max())).astype(int)\n",
        "  prev_10days_max = (df['close'] * is_10days_max).replace(to_replace=0, method='ffill').shift(6)\n",
        "  df['above_prev_10days_max'] = (df['close'] > prev_10days_max).astype(int)\n",
        "  is_10days_min = ((df['close'] == df['close'].rolling(window=6).min()) & (df['close'] == df['close'].shift(-5).rolling(window=6).min())).astype(int)\n",
        "  prev_10days_min = (df['close'] * is_10days_min).replace(to_replace=0, method='ffill').shift(6)\n",
        "  df['below_prev_10days_min'] = (df['close'] < prev_10days_min).astype(int)\n",
        "\n",
        "  df['above_prev_10days_min'] = (df['close'] > prev_10days_min).astype(int)\n",
        "  df['below_prev_10days_max'] = (df['close'] < prev_10days_max).astype(int)\n",
        "  \n",
        "  df['first_above_prev_10days_max'] = ((df['above_prev_10days_max'] == 1) & \\\n",
        "  (df['above_prev_10days_max'] + df['above_prev_10days_max'].shift(1) == 1)).astype(int)\n",
        "  df['first_below_prev_10days_min'] = ((df['below_prev_10days_min'] == 1) & \\\n",
        "  (df['below_prev_10days_min'] + df['below_prev_10days_min'].shift(1) == 1)).astype(int)\n",
        "  \n",
        "  df.drop(columns=['close'], inplace=True)\n",
        "  \n",
        "  if file[5:7] == currency[:2]:\n",
        "    df.columns = file[5:7] + file[3:5] + '_' + df.columns\n",
        "  else:\n",
        "    df.columns = file[3:7] + '_' + df.columns\n",
        "  print('features added finished: ', file)\n",
        "  #print(df.tail(5), df.isnull().sum())\n",
        "  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSNLu9JJWf3z"
      },
      "source": [
        "data = pd.DataFrame(columns=['DATE'])\n",
        "data = data.set_index('DATE')\n",
        "dir = os.getcwd()\n",
        "for f in os.listdir(dir):\n",
        "  #print(f)\n",
        "  if f.find(currency[:2]) != -1:\n",
        "    #print('----start ', f)\n",
        "    df = wrangle(f)\n",
        "    #print(data)\n",
        "    data = pd.concat([df, data], axis=1,)\n",
        "    #print('data: ', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fzk-yP1C8N1"
      },
      "source": [
        "# add more features to all data\n",
        "\n",
        "\n",
        "currency1_above_prev_10days_min_columns = [c for c in data.columns if c[5:] == 'above_prev_10days_min']\n",
        "currency1_below_prev_10days_max_columns = [c for c in data.columns if c[5:] == 'below_prev_10days_max']\n",
        "data['currency1_total_above_prev_10days_min'] = data[currency1_above_prev_10days_min_columns].sum(axis=1)\n",
        "data['currency1_total_below_prev_10days_max'] = data[currency1_below_prev_10days_max_columns].sum(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb9AR9cMdzlL"
      },
      "source": [
        "for f in os.listdir(dir):\n",
        "  if (f[-6:-4] == currency[2:]) & (f[3:7] != currency):\n",
        "    print(f)\n",
        "    df = wrangle(f)\n",
        "    #print(data.shape)\n",
        "    data = pd.concat([df, data], axis=1)\n",
        "    print(data.shape)\n",
        "print(data.shape, data.isnull().sum())\n",
        "print(data.tail(40))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09G4purPeR9q"
      },
      "source": [
        "# add more feature to all data\n",
        "currency1 = currency[:2]\n",
        "currency2 = currency[2:]\n",
        "\n",
        "data['total_positive_change'] = (data > 0)[[c for c in data.columns if (c.find('_change') != -1 & c.find('next') == -1)]].astype(int).sum(axis=1)\n",
        "data['total_negative_change'] = (data < 0)[[c for c in data.columns if (c.find('_change') != -1 & c.find('next') == -1)]].astype(int).sum(axis=1)\n",
        "data['total_currency1_up_change'] = (data > 0)[[c for c in data.columns if ((c.find('_change') != -1) & (c.find(currency1) != -1) & (c.find('next') == -1))]].astype(int).sum(axis=1)\n",
        "data['total_currency2_down_change'] = (data > 0)[[c for c in data.columns if (c.find(currency2 + '_change') != -1 & c.find('next') == -1)]].astype(int).sum(axis=1)\n",
        "\n",
        "currency2_above_prev_10days_min_columns = [c for c in data.columns if c[2:] == 'currency2_above_prev_10days_min']\n",
        "currency2_below_prev_10days_max_columns = [c for c in data.columns if c[2:] == 'currency2_below_prev_10days_max']\n",
        "data['currency2_total_above_prev_10days_min'] = data[currency2_above_prev_10days_min_columns].sum(axis=1)\n",
        "data['currency2_total_below_prev_10days_max'] = data[currency2_below_prev_10days_max_columns].sum(axis=1)\n",
        "\n",
        "data['total_above_prev_10days_min'] = data['currency1_total_above_prev_10days_min'] + data['currency2_total_above_prev_10days_min']\n",
        "data['total_below_prev_10days_max'] = data['currency1_total_below_prev_10days_max'] + data['currency2_total_below_prev_10days_max']\n",
        "'''\n",
        "bias_5days_columns = [c for c in data.columns if c.find('bias_5days') != -1]\n",
        "bias_10days_columns = [c for c in data.columns if c.find('bias_10days') != -1]\n",
        "bias_30days_columns = [c for c in data.columns if c.find('bias_30days') != -1]\n",
        "bias_45days_columns = [c for c in data.columns if c.find('bias_45days') != -1]\n",
        "bias_60days_columns = [c for c in data.columns if c.find('bias_60days') != -1]\n",
        "data['max_bias_5days'] = data[bias_5days_columns].max(axis=1)\n",
        "data['max_bias_10days'] = data[bias_10days_columns].max(axis=1)\n",
        "data['max_bias_30days'] = data[bias_30days_columns].max(axis=1)\n",
        "data['max_bias_45days'] = data[bias_45days_columns].max(axis=1)\n",
        "data['max_bias_60days'] = data[bias_60days_columns].max(axis=1)\n",
        "data['min_bias_5days'] = data[bias_5days_columns].min(axis=1)\n",
        "data['min_bias_10days'] = data[bias_10days_columns].min(axis=1)\n",
        "data['min_bias_30days'] = data[bias_30days_columns].min(axis=1)\n",
        "data['min_bias_45days'] = data[bias_45days_columns].min(axis=1)\n",
        "data['min_bias_60days'] = data[bias_60days_columns].min(axis=1)\n",
        "data['total_up_bias_5days'] = (data[bias_5days_columns] > 0).astype(int).sum(axis=1)\n",
        "data['total_up_bias_10days'] = (data[bias_10days_columns] > 0).astype(int).sum(axis=1)\n",
        "data['total_up_bias_30days'] = (data[bias_30days_columns] > 0).astype(int).sum(axis=1)\n",
        "data['total_up_bias_45days'] = (data[bias_45days_columns] > 0).astype(int).sum(axis=1)\n",
        "data['total_up_bias_60days'] = (data[bias_60days_columns] > 0).astype(int).sum(axis=1)\n",
        "data['total_mean_bias_5days'] = data[bias_5days_columns].mean(axis=1)\n",
        "data['total_mean_bias_10days'] = data[bias_10days_columns].mean(axis=1)\n",
        "data['total_mean_bias_30days'] = data[bias_30days_columns].mean(axis=1)\n",
        "data['total_mean_bias_45days'] = data[bias_45days_columns].mean(axis=1)\n",
        "data['total_mean_bias_60days'] = data[bias_60days_columns].mean(axis=1)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gs13TOPiPFt"
      },
      "source": [
        "#data.dropna(axis=0, thresh=100, inplace=True)\n",
        "print(data.shape, data.isnull().sum())\n",
        "data.dropna(axis=0, thresh= 60, inplace=True)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpiOtGDqerJe"
      },
      "source": [
        "data = data.iloc[60:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5joJuaFJ0eP"
      },
      "source": [
        "## Prepare training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx3JQf3tk7d_"
      },
      "source": [
        "data1 = data[data[currency + '_next5days_change'].notna()].copy()\n",
        "target = currency + '_next5days_change'\n",
        "X = data1.drop(columns=[c for c in data1.columns if c.find('next5days') != -1])\n",
        "y = data1[target]\n",
        "X.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOC2U__LlPR4"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYnNc4v1j-Rv"
      },
      "source": [
        "###target training data distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSMMQF6MRLrJ"
      },
      "source": [
        "y_train.plot(kind='hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcZ9P4og9d6V"
      },
      "source": [
        "# Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBg_AS2T7SLJ"
      },
      "source": [
        "## Baseline error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ESh0G197UOf"
      },
      "source": [
        "baseline = mean_absolute_error(y_train, [y_train.mean()] * len(y_train))\n",
        "print('Baseline error: ', baseline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b__qGh-Q966F"
      },
      "source": [
        "model_lr = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    Ridge(random_state=42)\n",
        ")\n",
        "model_lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKv_WkLukni3"
      },
      "source": [
        "## Ridge absolute error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYDwXXdq97LM"
      },
      "source": [
        "print(\"train error: \", mean_absolute_error(y_train, model_lr.predict(X_train)))\n",
        "print(\"validation error: \", mean_absolute_error(y_val, model_lr.predict(X_val)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP-ENBxYTKom"
      },
      "source": [
        "## Tuning Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0S_GVjBM3o3"
      },
      "source": [
        "clf = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    Ridge(random_state=42),\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    #'simpleimputer__strategy': ['mean', 'median'],\n",
        "    'ridge__random_state': range(30,33,1),\n",
        "    #'ridge__max_iter': range(1,20,2),\n",
        "    'ridge__alpha': np.arange(16000,20000,20)\n",
        "}\n",
        "\n",
        "model_lrrs = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_distributions = param_grid,\n",
        "    n_jobs = -1,\n",
        "    cv = 10,\n",
        "    verbose = 1,\n",
        "    n_iter = 200\n",
        ")\n",
        "\n",
        "model_lrrs.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85EyP-hcPSL-"
      },
      "source": [
        "best_score = model_lrrs.best_score_\n",
        "best_params = model_lrrs.best_params_\n",
        "\n",
        "print('Best score for `model`:', best_score)\n",
        "print('Best params for `model`:', best_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34bhF7wkzfq"
      },
      "source": [
        "###Tuned Ridge absolute error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZikmIjvXO6NO"
      },
      "source": [
        "train_error = mean_absolute_error(y_train, model_lrrs.predict(X_train))\n",
        "validation_error = mean_absolute_error(y_val, model_lrrs.predict(X_val))\n",
        "print(\"train error: \", train_error)\n",
        "print(\"validation error: \", validation_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD-sak7rRw34"
      },
      "source": [
        "# Classified models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBvwKuIIX8xs"
      },
      "source": [
        "## prepare classified target: -1, 0 or 1 (sell, non, buy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iyn3djzYDVK"
      },
      "source": [
        "## Baseline score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v08Fp1vRWfMU"
      },
      "source": [
        "y_logistic = data1[target].apply(lambda x: -1 if x <= -0.005 else(0 if x < 0.005 else 1))\n",
        "X_train, X_val, y_train_c, y_val_c = train_test_split(X, y_logistic, test_size=0.2)\n",
        "X_train.dtypes, y_train_c.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynzaFWvJXvyk"
      },
      "source": [
        "baseline = y_train_c.value_counts(normalize=True).max()\n",
        "print('Baseline: ', baseline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHETGiTWVGDR"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M1DoWBGVGDR"
      },
      "source": [
        "model_lg = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(random_state=42, n_jobs=-1)\n",
        ")\n",
        "model_lg.fit(X_train, y_train_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYRvmRQzlICC"
      },
      "source": [
        "##Acuuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa6vSxZEVGDS"
      },
      "source": [
        "print(\"train accuracy_score: \", accuracy_score(y_train_c, model_lg.predict(X_train)))\n",
        "print(\"validation accuracy_score: \", accuracy_score(y_val_c, model_lg.predict(X_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZsSkuD_VGDS"
      },
      "source": [
        "## Tuning LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqTC8YcVGDS"
      },
      "source": [
        "clf = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(random_state=42, n_jobs=-1),\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "   'logisticregression__max_iter': range(60, 400, 10),\n",
        "}\n",
        "\n",
        "model_lgrs = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_distributions = param_grid,\n",
        "    n_jobs = -1,\n",
        "    cv = 10,\n",
        "    verbose = 1,\n",
        "    n_iter = 200\n",
        ")\n",
        "\n",
        "model_lgrs.fit(X_train, y_train_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTrDDM2nVGDS"
      },
      "source": [
        "best_score = model_lgrs.best_score_\n",
        "best_params = model_lgrs.best_params_\n",
        "\n",
        "print('Best score for `model`:', best_score)\n",
        "print('Best params for `model`:', best_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T6EXmrnlLvX"
      },
      "source": [
        "###Tuned accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VX3H3wUVGDS"
      },
      "source": [
        "train_accuracy = accuracy_score(y_train_c, model_lgrs.predict(X_train))\n",
        "validation_accuracy = accuracy_score(y_val_c, model_lgrs.predict(X_val))\n",
        "print(\"train accuracy: \", train_accuracy)\n",
        "print(\"validation accuracy: \", validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoQAXeaxp9yS"
      },
      "source": [
        "# RandomForest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVUB2YcRSEgr"
      },
      "source": [
        "model_rf = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(n_jobs=-1,\n",
        "                           random_state=42)\n",
        ")\n",
        "\n",
        "model_rf.fit(X_train, y_train_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eugJe0GylTHr"
      },
      "source": [
        "##Accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM7VKBphSOdg"
      },
      "source": [
        "train_accuracy = accuracy_score(y_train_c, model_rf.predict(X_train))\n",
        "validation_accuracy = accuracy_score(y_val_c, model_rf.predict(X_val))\n",
        "print(\"train accuracy: \", train_accuracy)\n",
        "print(\"validation accuracy: \", validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMaRYEWZP5c6"
      },
      "source": [
        "X.select_dtypes(include='object')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lgXzrprjH8M"
      },
      "source": [
        "## Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAHKvFEc_ks3"
      },
      "source": [
        "importances = model_rf.named_steps['randomforestclassifier'].feature_importances_\n",
        "columns = X_train.columns\n",
        "df_importances = pd.DataFrame(data=importances, index=X_train.columns, columns=[\"importance\"])\n",
        "df_importances.abs().sort_values(by=['importance']).tail(10).plot(kind=\"barh\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGOSgyL7cBc6"
      },
      "source": [
        "df_importances.abs().sort_values(by=['importance'])[-40:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pckTJuVlS4bT"
      },
      "source": [
        "## Tuning RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjjMiAKJArtl"
      },
      "source": [
        "clf = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    #'randomforestclassifier__random_state': range(30,60,2),\n",
        "    'randomforestclassifier__n_estimators': range(400, 600, 10),\n",
        "    'randomforestclassifier__max_depth': range(5,40,2),\n",
        "    #'randomforestclassifier__min_samples_split': range(3,8,1),\n",
        "    #'randomforestclassifier__max_features': range(22,40,1)\n",
        "}\n",
        "\n",
        "model_rfrs = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_distributions = param_grid,\n",
        "    n_jobs = -1,\n",
        "    cv = 5,\n",
        "    verbose = 1,\n",
        "    n_iter = 50\n",
        ")\n",
        "\n",
        "model_rfrs.fit(X_train, y_train_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFL6dLnWIx97"
      },
      "source": [
        "best_score = model_rfrs.best_score_\n",
        "best_params = model_rfrs.best_params_\n",
        "\n",
        "print('Best score for `model`:', best_score)\n",
        "print('Best params for `model`:', best_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWLaxwA-lXTN"
      },
      "source": [
        "###Tuned accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTRyWQ2zArtn"
      },
      "source": [
        "train_accuracy = accuracy_score(y_train_c, model_rfrs.predict(X_train))\n",
        "validation_accuracy = accuracy_score(y_val_c, model_rfrs.predict(X_val))\n",
        "print(\"train accuracy: \", train_accuracy)\n",
        "print(\"validation accuracy: \", validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf0EmPzEmGx4"
      },
      "source": [
        "# Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0f1DhM1mFl9"
      },
      "source": [
        "model_xgb = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
        ")\n",
        "\n",
        "model_xgb.fit(X_train, y_train_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWHrvFgzttgX"
      },
      "source": [
        "##Accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XalgvzJYm8XN"
      },
      "source": [
        "train_accuracy = accuracy_score(y_train_c, model_xgb.predict(X_train))\n",
        "validation_accuracy = accuracy_score(y_val_c, model_xgb.predict(X_val))\n",
        "print(\"train accuracy: \", train_accuracy)\n",
        "print(\"validation accuracy: \", validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm6ipHNdIJKK"
      },
      "source": [
        "## Tuning Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqiEq62rG2yE"
      },
      "source": [
        "clf = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    GradientBoostingClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    #'randomforestclassifier__random_state': range(30,60,2),\n",
        "    'gradientboostingclassifier__n_estimators': range(80, 120, 10),\n",
        "    #'gradientboostingclassifier__learning_rate': np.arange(0,0.2,0.02),\n",
        "    'gradientboostingclassifier__max_depth': range(10,20,1),\n",
        "    #'gradientboostingclassifier__max_features': range(10,40,2)\n",
        "}\n",
        "\n",
        "model_gbrs = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_distributions = param_grid,\n",
        "    n_jobs = -1,\n",
        "    cv = 5,\n",
        "    verbose = 1,\n",
        "    n_iter = 100\n",
        ")\n",
        "\n",
        "model_gbrs.fit(X_train, y_train_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTAdj0juG2yE"
      },
      "source": [
        "best_score = model_gbrs.best_score_\n",
        "best_params = model_gbrs.best_params_\n",
        "\n",
        "print('Best score for `model`:', best_score)\n",
        "print('Best params for `model`:', best_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anCzILulG2yE"
      },
      "source": [
        "###Tuned accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZvYypfPG2yF"
      },
      "source": [
        "train_accuracy = accuracy_score(y_train_c, model_gbrs.predict(X_train))\n",
        "validation_accuracy = accuracy_score(y_val_c, model_gbrs.predict(X_val))\n",
        "print(\"train accuracy: \", train_accuracy)\n",
        "print(\"validation accuracy: \", validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRMp_VP1QcCa"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM3xs9C2yqSG"
      },
      "source": [
        "plot_confusion_matrix(\n",
        "    model_gbrs,\n",
        "    X_val,\n",
        "    y_val_c,\n",
        "    values_format = '.0f',\n",
        "    display_labels = ['sell', 'None', 'buy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhZICN70Qgkf"
      },
      "source": [
        "#### Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M-yYZ39QsEe"
      },
      "source": [
        "print('Sell precision: ', 151/(150 + 35 + 32))\n",
        "print('Buy precision: ', 137/(137 + 36 + 50))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}